{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gasia/anaconda3/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import glob\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_space_df = pd.read_csv('/home/gasia/StarSpace/words_space.tsv', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>2.306850e-04</td>\n",
       "      <td>-0.002487</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>8.903410e-04</td>\n",
       "      <td>-0.000867</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>-8.548350e-04</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000868</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>-0.002038</td>\n",
       "      <td>-0.001416</td>\n",
       "      <td>-0.001188</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>-1.039700e-03</td>\n",
       "      <td>1.249780e-03</td>\n",
       "      <td>-0.000796</td>\n",
       "      <td>-8.301510e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__word__price</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>2.973840e-06</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>2.896700e-05</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>4.506230e-07</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-4.357470e-05</td>\n",
       "      <td>2.011180e-05</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-7.639540e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__word__go</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>-4.015650e-06</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>-6.160200e-06</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>-4.459280e-05</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-1.563940e-05</td>\n",
       "      <td>1.153700e-05</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-2.430360e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__word__people</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>1.171530e-05</td>\n",
       "      <td>-0.000140</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-1.690560e-05</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-1.161040e-04</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>1.222220e-05</td>\n",
       "      <td>-5.887540e-05</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>3.758550e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__word__time</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>-4.201780e-05</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-3.616490e-05</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-2.569730e-05</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-3.797110e-05</td>\n",
       "      <td>-1.402450e-05</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>-6.772960e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>__word__rise</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-1.503780e-05</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>7.164010e-05</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>1.904780e-07</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-8.350000e-05</td>\n",
       "      <td>6.057710e-05</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>-4.780960e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>__word__buy</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>-6.503720e-05</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-3.680070e-05</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>-9.256760e-05</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000095</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-6.154720e-06</td>\n",
       "      <td>-5.206500e-06</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-1.546800e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>__word__good</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-1.697910e-05</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-5.420950e-06</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-6.218520e-05</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-3.421030e-05</td>\n",
       "      <td>4.295350e-05</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>1.250570e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>__word__year</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>-4.497130e-05</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>9.840050e-05</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>2.076430e-05</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000104</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>2.274020e-05</td>\n",
       "      <td>8.231730e-05</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>-1.743520e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>__word__see</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>3.179700e-07</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>2.281250e-05</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-5.124530e-05</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>4.706920e-06</td>\n",
       "      <td>-5.948090e-05</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-5.747050e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>__word__market</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-8.552860e-05</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>-1.088330e-04</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>-1.135720e-05</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000116</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-9.676540e-05</td>\n",
       "      <td>8.301950e-06</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>1.810370e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>__word__make</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-8.568250e-06</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-5.899240e-05</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000081</td>\n",
       "      <td>-5.015670e-05</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>4.682290e-05</td>\n",
       "      <td>5.608910e-05</td>\n",
       "      <td>-0.000106</td>\n",
       "      <td>8.471740e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>__word__happen</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>4.278960e-05</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.716380e-05</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-7.456680e-05</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>1.202550e-05</td>\n",
       "      <td>1.455940e-05</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-8.176780e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>__word__like</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>-7.956410e-06</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>2.103190e-05</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-1.196680e-04</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>5.423480e-05</td>\n",
       "      <td>-3.767350e-05</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>2.584870e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>__word__money</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-5.086750e-05</td>\n",
       "      <td>-0.000171</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>-1.015540e-04</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-1.142120e-04</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-0.000143</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>1.277130e-04</td>\n",
       "      <td>6.132710e-05</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>1.761470e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>__word__know</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-6.000350e-06</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-4.460470e-05</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-2.935990e-05</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>-1.164980e-07</td>\n",
       "      <td>-6.795670e-05</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-5.392040e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>__word__one</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-7.904870e-05</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>2.324080e-05</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-6.081730e-05</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>4.326830e-05</td>\n",
       "      <td>4.024570e-05</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-2.255610e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>__word__sell</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>-1.669280e-05</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-4.551940e-05</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>-3.047290e-05</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>-0.000085</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-2.725490e-05</td>\n",
       "      <td>6.437440e-05</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>5.761320e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>__word__high</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>-2.978150e-05</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-9.969450e-07</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-1.119710e-05</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>-6.702450e-05</td>\n",
       "      <td>3.389700e-05</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>6.228510e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>__word__bitcoin</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>1.934590e-04</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>-0.000110</td>\n",
       "      <td>4.661170e-05</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>-2.217640e-04</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>1.007680e-04</td>\n",
       "      <td>4.726460e-05</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>-1.575370e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>__word__even</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>2.802450e-05</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>2.240520e-05</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>2.252010e-05</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>-3.407370e-05</td>\n",
       "      <td>1.734660e-06</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-8.174900e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>__word__still</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-3.306890e-05</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000173</td>\n",
       "      <td>4.552790e-05</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>5.556940e-05</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000132</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-4.815830e-05</td>\n",
       "      <td>4.127900e-06</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>-9.324580e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>__word__also</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>3.451810e-05</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>-2.887120e-05</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-6.824580e-05</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-2.972300e-06</td>\n",
       "      <td>4.834600e-05</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>5.954240e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>__word__don</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>8.575820e-05</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>-1.258500e-05</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-7.119820e-05</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>5.082580e-06</td>\n",
       "      <td>-1.365750e-04</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-4.572480e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>__word__coin</td>\n",
       "      <td>-0.000116</td>\n",
       "      <td>-3.001440e-04</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>-7.908060e-05</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-1.419140e-04</td>\n",
       "      <td>-0.000213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>-1.975670e-04</td>\n",
       "      <td>5.710110e-05</td>\n",
       "      <td>-0.000120</td>\n",
       "      <td>5.518170e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>__word__value</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>7.626980e-05</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>-1.754790e-05</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-2.681780e-05</td>\n",
       "      <td>-0.000103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000229</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>3.797800e-05</td>\n",
       "      <td>9.211330e-08</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>-2.054590e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>__word__currency</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>-5.104700e-05</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>-1.194060e-04</td>\n",
       "      <td>-0.000181</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>-1.096660e-04</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000219</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>-0.000203</td>\n",
       "      <td>-0.000264</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>2.212090e-06</td>\n",
       "      <td>2.685890e-05</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>-6.131520e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>__word__many</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-3.920700e-05</td>\n",
       "      <td>-0.000122</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>2.812940e-05</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>-1.046650e-04</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>-3.293670e-05</td>\n",
       "      <td>-3.160790e-05</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>5.073330e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>__word__long</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>2.467870e-05</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>4.845060e-05</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-3.372640e-05</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>-0.000081</td>\n",
       "      <td>-1.465590e-04</td>\n",
       "      <td>-6.357790e-05</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-1.378800e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>__word__really</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>3.799370e-07</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>-0.000165</td>\n",
       "      <td>2.802190e-05</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>-7.411120e-06</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>1.114940e-04</td>\n",
       "      <td>-5.830420e-05</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>-4.967930e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78961</th>\n",
       "      <td>__word__fuckaling</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>-2.620640e-03</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>-0.001182</td>\n",
       "      <td>-2.168760e-03</td>\n",
       "      <td>-0.000510</td>\n",
       "      <td>-0.002193</td>\n",
       "      <td>1.836560e-03</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001063</td>\n",
       "      <td>0.001047</td>\n",
       "      <td>-0.002676</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>-0.001325</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>-7.991940e-05</td>\n",
       "      <td>6.953040e-04</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>1.005080e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78962</th>\n",
       "      <td>__word__eople</td>\n",
       "      <td>-0.001287</td>\n",
       "      <td>1.048680e-03</td>\n",
       "      <td>-0.001665</td>\n",
       "      <td>-0.001390</td>\n",
       "      <td>-2.108740e-04</td>\n",
       "      <td>-0.001398</td>\n",
       "      <td>0.001089</td>\n",
       "      <td>1.231100e-03</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000432</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>-0.001453</td>\n",
       "      <td>-0.002717</td>\n",
       "      <td>-0.003381</td>\n",
       "      <td>-0.001546</td>\n",
       "      <td>2.753840e-04</td>\n",
       "      <td>-1.608610e-04</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>-1.380220e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78963</th>\n",
       "      <td>__word__aoon</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>4.815000e-04</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>-0.000837</td>\n",
       "      <td>-1.127690e-03</td>\n",
       "      <td>0.002258</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>-9.777360e-04</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000950</td>\n",
       "      <td>-0.001155</td>\n",
       "      <td>-0.000083</td>\n",
       "      <td>-0.000600</td>\n",
       "      <td>-0.000359</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>1.474880e-03</td>\n",
       "      <td>1.524700e-03</td>\n",
       "      <td>-0.000450</td>\n",
       "      <td>-1.085590e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78964</th>\n",
       "      <td>__word__stardate</td>\n",
       "      <td>-0.000558</td>\n",
       "      <td>-2.062710e-03</td>\n",
       "      <td>-0.001540</td>\n",
       "      <td>-0.001117</td>\n",
       "      <td>9.955040e-04</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>-0.001055</td>\n",
       "      <td>1.824730e-03</td>\n",
       "      <td>0.002824</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001912</td>\n",
       "      <td>-0.000733</td>\n",
       "      <td>-0.000808</td>\n",
       "      <td>0.002837</td>\n",
       "      <td>0.002194</td>\n",
       "      <td>0.003074</td>\n",
       "      <td>3.532140e-05</td>\n",
       "      <td>-1.245350e-03</td>\n",
       "      <td>-0.000844</td>\n",
       "      <td>-1.443350e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78965</th>\n",
       "      <td>__word__fatboy</td>\n",
       "      <td>-0.001363</td>\n",
       "      <td>-9.426400e-04</td>\n",
       "      <td>-0.000525</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>-1.551090e-04</td>\n",
       "      <td>0.001986</td>\n",
       "      <td>-0.001082</td>\n",
       "      <td>5.994700e-04</td>\n",
       "      <td>-0.001049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.002484</td>\n",
       "      <td>0.004016</td>\n",
       "      <td>-3.046230e-05</td>\n",
       "      <td>-2.057460e-03</td>\n",
       "      <td>-0.000630</td>\n",
       "      <td>-2.503440e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78966</th>\n",
       "      <td>__word__nabob</td>\n",
       "      <td>-0.002292</td>\n",
       "      <td>-7.951020e-04</td>\n",
       "      <td>-0.001822</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>1.390690e-03</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>-0.003352</td>\n",
       "      <td>-9.738830e-05</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>-0.001012</td>\n",
       "      <td>0.001441</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.002915</td>\n",
       "      <td>6.809090e-04</td>\n",
       "      <td>-2.607660e-03</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>-5.024280e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78967</th>\n",
       "      <td>__word__bjp</td>\n",
       "      <td>-0.000667</td>\n",
       "      <td>-1.633440e-03</td>\n",
       "      <td>-0.002409</td>\n",
       "      <td>0.000931</td>\n",
       "      <td>5.747870e-04</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>-0.000794</td>\n",
       "      <td>-1.224550e-03</td>\n",
       "      <td>0.001605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>-0.000648</td>\n",
       "      <td>0.001867</td>\n",
       "      <td>0.003074</td>\n",
       "      <td>0.002686</td>\n",
       "      <td>0.002670</td>\n",
       "      <td>6.179370e-04</td>\n",
       "      <td>-2.472540e-03</td>\n",
       "      <td>0.002468</td>\n",
       "      <td>-3.443540e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78968</th>\n",
       "      <td>__word__eedddbabeeefadfddbcccbe</td>\n",
       "      <td>-0.000426</td>\n",
       "      <td>1.016100e-04</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>-0.001327</td>\n",
       "      <td>-1.590300e-04</td>\n",
       "      <td>-0.001064</td>\n",
       "      <td>-0.000947</td>\n",
       "      <td>-4.533870e-04</td>\n",
       "      <td>-0.000355</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001863</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-0.001914</td>\n",
       "      <td>0.002784</td>\n",
       "      <td>-0.002119</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>1.732900e-03</td>\n",
       "      <td>-2.225020e-03</td>\n",
       "      <td>-0.000647</td>\n",
       "      <td>-1.470310e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78969</th>\n",
       "      <td>__word__cqkby</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>4.224240e-05</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>-0.001691</td>\n",
       "      <td>1.667710e-03</td>\n",
       "      <td>-0.003467</td>\n",
       "      <td>0.001803</td>\n",
       "      <td>-1.431730e-03</td>\n",
       "      <td>-0.000203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000938</td>\n",
       "      <td>-0.003094</td>\n",
       "      <td>-0.002979</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>-0.000513</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>-4.302520e-04</td>\n",
       "      <td>-1.313020e-04</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>7.231630e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78970</th>\n",
       "      <td>__word__huhh</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>-3.858930e-04</td>\n",
       "      <td>-0.001691</td>\n",
       "      <td>-0.000520</td>\n",
       "      <td>-6.876020e-04</td>\n",
       "      <td>-0.002378</td>\n",
       "      <td>-0.000701</td>\n",
       "      <td>-2.563880e-03</td>\n",
       "      <td>-0.002059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>-0.000801</td>\n",
       "      <td>0.001175</td>\n",
       "      <td>-0.001148</td>\n",
       "      <td>-1.221400e-03</td>\n",
       "      <td>-2.093840e-04</td>\n",
       "      <td>-0.004070</td>\n",
       "      <td>-1.077560e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78971</th>\n",
       "      <td>__word__significantl</td>\n",
       "      <td>0.002670</td>\n",
       "      <td>-1.905960e-03</td>\n",
       "      <td>0.000973</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>9.084450e-04</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>-0.001860</td>\n",
       "      <td>1.333330e-03</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>-0.000452</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>0.000973</td>\n",
       "      <td>-0.000217</td>\n",
       "      <td>9.564120e-04</td>\n",
       "      <td>1.875080e-03</td>\n",
       "      <td>-0.001768</td>\n",
       "      <td>-2.778280e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78972</th>\n",
       "      <td>__word__viant</td>\n",
       "      <td>-0.001340</td>\n",
       "      <td>-3.159160e-03</td>\n",
       "      <td>0.002478</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>1.237350e-03</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>-1.586510e-03</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001355</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>-0.004386</td>\n",
       "      <td>-0.000741</td>\n",
       "      <td>-0.000873</td>\n",
       "      <td>-0.000412</td>\n",
       "      <td>-1.079340e-03</td>\n",
       "      <td>-1.037770e-03</td>\n",
       "      <td>-0.002255</td>\n",
       "      <td>-1.567260e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78973</th>\n",
       "      <td>__word__faull</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>-1.992730e-03</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>-0.001585</td>\n",
       "      <td>-1.357520e-03</td>\n",
       "      <td>-0.000219</td>\n",
       "      <td>-0.001246</td>\n",
       "      <td>-1.767200e-03</td>\n",
       "      <td>0.001626</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001054</td>\n",
       "      <td>-0.001224</td>\n",
       "      <td>-0.002171</td>\n",
       "      <td>0.001704</td>\n",
       "      <td>-0.000417</td>\n",
       "      <td>0.002698</td>\n",
       "      <td>-9.265140e-04</td>\n",
       "      <td>-2.299090e-04</td>\n",
       "      <td>-0.001519</td>\n",
       "      <td>-7.929800e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78974</th>\n",
       "      <td>__word__strengthing</td>\n",
       "      <td>0.001906</td>\n",
       "      <td>-1.644840e-03</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>-0.000566</td>\n",
       "      <td>-2.000040e-04</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>1.723080e-04</td>\n",
       "      <td>-0.002437</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002486</td>\n",
       "      <td>-0.001053</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.001521</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>2.477130e-03</td>\n",
       "      <td>-5.135250e-05</td>\n",
       "      <td>-0.000675</td>\n",
       "      <td>-1.663800e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78975</th>\n",
       "      <td>__word__buybitcoin</td>\n",
       "      <td>0.001752</td>\n",
       "      <td>2.305100e-03</td>\n",
       "      <td>-0.000603</td>\n",
       "      <td>-0.001633</td>\n",
       "      <td>5.395210e-04</td>\n",
       "      <td>-0.001625</td>\n",
       "      <td>-0.001842</td>\n",
       "      <td>1.156650e-03</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>-0.003280</td>\n",
       "      <td>0.002624</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>-2.825110e-04</td>\n",
       "      <td>1.912130e-04</td>\n",
       "      <td>-0.001442</td>\n",
       "      <td>1.166860e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78976</th>\n",
       "      <td>__word__netqk</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>1.526940e-03</td>\n",
       "      <td>-0.000789</td>\n",
       "      <td>-0.000559</td>\n",
       "      <td>6.881080e-04</td>\n",
       "      <td>-0.000556</td>\n",
       "      <td>-0.000924</td>\n",
       "      <td>6.732900e-04</td>\n",
       "      <td>-0.001681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>-0.000499</td>\n",
       "      <td>-0.000255</td>\n",
       "      <td>-0.000138</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>-1.363180e-03</td>\n",
       "      <td>-1.091070e-03</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>2.802170e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78977</th>\n",
       "      <td>__word__jammalan</td>\n",
       "      <td>-0.002422</td>\n",
       "      <td>1.973180e-03</td>\n",
       "      <td>-0.000510</td>\n",
       "      <td>-0.000935</td>\n",
       "      <td>1.145050e-03</td>\n",
       "      <td>-0.001999</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>-7.002450e-04</td>\n",
       "      <td>0.002785</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>-0.001023</td>\n",
       "      <td>-0.002352</td>\n",
       "      <td>-0.000746</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.002133</td>\n",
       "      <td>-1.206440e-03</td>\n",
       "      <td>-2.821130e-04</td>\n",
       "      <td>-0.001127</td>\n",
       "      <td>-1.360350e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78978</th>\n",
       "      <td>__word__touristing</td>\n",
       "      <td>-0.001092</td>\n",
       "      <td>2.200600e-03</td>\n",
       "      <td>-0.000692</td>\n",
       "      <td>-0.001479</td>\n",
       "      <td>9.945140e-04</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>-0.003104</td>\n",
       "      <td>-1.664580e-03</td>\n",
       "      <td>-0.001810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>-0.000505</td>\n",
       "      <td>-0.001069</td>\n",
       "      <td>-0.000977</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>-2.789180e-03</td>\n",
       "      <td>-1.972160e-03</td>\n",
       "      <td>-0.001505</td>\n",
       "      <td>-5.836550e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78979</th>\n",
       "      <td>__word__wxngrrjhelu</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>1.573170e-03</td>\n",
       "      <td>-0.000255</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>-2.026220e-03</td>\n",
       "      <td>-0.000138</td>\n",
       "      <td>-0.001334</td>\n",
       "      <td>-2.687830e-05</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000774</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>-0.001245</td>\n",
       "      <td>-0.001289</td>\n",
       "      <td>-0.000539</td>\n",
       "      <td>-1.497370e-03</td>\n",
       "      <td>-1.036310e-03</td>\n",
       "      <td>-0.001056</td>\n",
       "      <td>-9.411020e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78980</th>\n",
       "      <td>__word__woooaaa</td>\n",
       "      <td>0.001943</td>\n",
       "      <td>-6.098240e-04</td>\n",
       "      <td>0.001454</td>\n",
       "      <td>-0.000369</td>\n",
       "      <td>2.648760e-03</td>\n",
       "      <td>-0.000970</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>-5.739110e-04</td>\n",
       "      <td>-0.000659</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002499</td>\n",
       "      <td>-0.001729</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>-0.002637</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>-0.001054</td>\n",
       "      <td>9.044390e-04</td>\n",
       "      <td>-2.354210e-04</td>\n",
       "      <td>-0.001018</td>\n",
       "      <td>-3.219100e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78981</th>\n",
       "      <td>__word__bles</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>1.040230e-03</td>\n",
       "      <td>-0.001060</td>\n",
       "      <td>-0.002453</td>\n",
       "      <td>-8.893930e-04</td>\n",
       "      <td>-0.001766</td>\n",
       "      <td>0.002186</td>\n",
       "      <td>5.628800e-05</td>\n",
       "      <td>-0.001417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>-0.000540</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>-0.001577</td>\n",
       "      <td>-0.003603</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>-1.140140e-03</td>\n",
       "      <td>1.910580e-03</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>5.976540e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78982</th>\n",
       "      <td>__word__dickeater</td>\n",
       "      <td>-0.001418</td>\n",
       "      <td>1.613910e-03</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>-1.188980e-03</td>\n",
       "      <td>-0.001594</td>\n",
       "      <td>-0.001036</td>\n",
       "      <td>-4.906060e-05</td>\n",
       "      <td>-0.002260</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001401</td>\n",
       "      <td>-0.001132</td>\n",
       "      <td>-0.002322</td>\n",
       "      <td>0.002753</td>\n",
       "      <td>0.002409</td>\n",
       "      <td>-0.001146</td>\n",
       "      <td>-3.656990e-04</td>\n",
       "      <td>-4.297690e-04</td>\n",
       "      <td>-0.001808</td>\n",
       "      <td>4.614080e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78983</th>\n",
       "      <td>__word__mvpmbq</td>\n",
       "      <td>-0.000185</td>\n",
       "      <td>3.572710e-04</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-1.749650e-03</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>-0.000169</td>\n",
       "      <td>2.765820e-04</td>\n",
       "      <td>0.002027</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000351</td>\n",
       "      <td>-0.001194</td>\n",
       "      <td>-0.002549</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>-0.004459</td>\n",
       "      <td>-0.002380</td>\n",
       "      <td>2.801240e-03</td>\n",
       "      <td>2.349990e-05</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>-2.223390e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78984</th>\n",
       "      <td>__word__segwitshit</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>-1.371230e-03</td>\n",
       "      <td>0.001933</td>\n",
       "      <td>0.002346</td>\n",
       "      <td>-1.738810e-03</td>\n",
       "      <td>-0.002046</td>\n",
       "      <td>0.002466</td>\n",
       "      <td>-1.147520e-03</td>\n",
       "      <td>-0.002247</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000538</td>\n",
       "      <td>-0.002259</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>-0.002065</td>\n",
       "      <td>0.002164</td>\n",
       "      <td>-1.401510e-04</td>\n",
       "      <td>2.481260e-03</td>\n",
       "      <td>-0.000318</td>\n",
       "      <td>-1.579690e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78985</th>\n",
       "      <td>__word__pseudosolution</td>\n",
       "      <td>0.002385</td>\n",
       "      <td>-1.901810e-04</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>0.002422</td>\n",
       "      <td>-3.860420e-04</td>\n",
       "      <td>-0.001809</td>\n",
       "      <td>0.001462</td>\n",
       "      <td>-3.688650e-03</td>\n",
       "      <td>-0.002483</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000816</td>\n",
       "      <td>-0.002545</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.003027</td>\n",
       "      <td>-0.001891</td>\n",
       "      <td>0.002263</td>\n",
       "      <td>-2.271970e-03</td>\n",
       "      <td>1.656410e-03</td>\n",
       "      <td>-0.001163</td>\n",
       "      <td>-2.956470e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78986</th>\n",
       "      <td>__word__gmn</td>\n",
       "      <td>-0.001890</td>\n",
       "      <td>5.482230e-04</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>-0.000097</td>\n",
       "      <td>-1.437860e-03</td>\n",
       "      <td>-0.000999</td>\n",
       "      <td>-0.000371</td>\n",
       "      <td>4.123920e-04</td>\n",
       "      <td>-0.001373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004555</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.002189</td>\n",
       "      <td>-0.002355</td>\n",
       "      <td>-0.000678</td>\n",
       "      <td>2.456270e-03</td>\n",
       "      <td>-1.055290e-03</td>\n",
       "      <td>-0.000333</td>\n",
       "      <td>4.490640e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78987</th>\n",
       "      <td>__word__bqrxqq</td>\n",
       "      <td>-0.002729</td>\n",
       "      <td>1.984420e-04</td>\n",
       "      <td>-0.000281</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>-9.748660e-04</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>-0.001724</td>\n",
       "      <td>-2.079110e-03</td>\n",
       "      <td>-0.001042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.002144</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>-0.001110</td>\n",
       "      <td>-0.002782</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>-1.376350e-03</td>\n",
       "      <td>5.834340e-04</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>2.126130e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78988</th>\n",
       "      <td>__word__nocoinersbtfoo</td>\n",
       "      <td>-0.002117</td>\n",
       "      <td>-4.120080e-03</td>\n",
       "      <td>-0.000378</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>-1.104440e-05</td>\n",
       "      <td>-0.001101</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>-6.928000e-04</td>\n",
       "      <td>-0.000447</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000816</td>\n",
       "      <td>-0.000147</td>\n",
       "      <td>0.000659</td>\n",
       "      <td>0.001360</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>-0.001595</td>\n",
       "      <td>-6.368150e-04</td>\n",
       "      <td>-3.036520e-04</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>4.633280e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78989</th>\n",
       "      <td>__word__wcasy</td>\n",
       "      <td>-0.000874</td>\n",
       "      <td>-5.617070e-04</td>\n",
       "      <td>0.002005</td>\n",
       "      <td>0.001123</td>\n",
       "      <td>-1.120120e-04</td>\n",
       "      <td>-0.001738</td>\n",
       "      <td>-0.000820</td>\n",
       "      <td>1.784230e-03</td>\n",
       "      <td>-0.003275</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000115</td>\n",
       "      <td>-0.001132</td>\n",
       "      <td>-0.002303</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.001237</td>\n",
       "      <td>-0.000496</td>\n",
       "      <td>8.010320e-04</td>\n",
       "      <td>-1.125800e-03</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>-2.823520e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78990</th>\n",
       "      <td>__word__dippening</td>\n",
       "      <td>-0.000080</td>\n",
       "      <td>-2.211080e-03</td>\n",
       "      <td>-0.000238</td>\n",
       "      <td>-0.001184</td>\n",
       "      <td>-8.126880e-04</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>-0.004313</td>\n",
       "      <td>6.895810e-04</td>\n",
       "      <td>0.000910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001923</td>\n",
       "      <td>0.001714</td>\n",
       "      <td>-0.000897</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>-0.001341</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>-2.494900e-04</td>\n",
       "      <td>1.547960e-03</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>7.585590e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78991 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    0         1             2         3   \\\n",
       "0                                  NaN  0.000891  2.306850e-04 -0.002487   \n",
       "1                        __word__price  0.000006  2.973840e-06  0.000004   \n",
       "2                           __word__go  0.000026 -4.015650e-06  0.000006   \n",
       "3                       __word__people -0.000029  1.171530e-05 -0.000140   \n",
       "4                         __word__time  0.000044 -4.201780e-05  0.000069   \n",
       "5                         __word__rise -0.000032 -1.503780e-05 -0.000033   \n",
       "6                          __word__buy  0.000104 -6.503720e-05  0.000020   \n",
       "7                         __word__good -0.000022 -1.697910e-05  0.000031   \n",
       "8                         __word__year  0.000040 -4.497130e-05  0.000001   \n",
       "9                          __word__see -0.000040  3.179700e-07  0.000015   \n",
       "10                      __word__market -0.000082 -8.552860e-05 -0.000013   \n",
       "11                        __word__make  0.000003 -8.568250e-06 -0.000011   \n",
       "12                      __word__happen -0.000008  4.278960e-05 -0.000029   \n",
       "13                        __word__like  0.000020 -7.956410e-06 -0.000041   \n",
       "14                       __word__money  0.000025 -5.086750e-05 -0.000171   \n",
       "15                        __word__know -0.000027 -6.000350e-06 -0.000048   \n",
       "16                         __word__one -0.000005 -7.904870e-05 -0.000005   \n",
       "17                        __word__sell  0.000104 -1.669280e-05 -0.000024   \n",
       "18                        __word__high  0.000035 -2.978150e-05  0.000031   \n",
       "19                     __word__bitcoin  0.000340  1.934590e-04  0.000168   \n",
       "20                        __word__even  0.000052  2.802450e-05 -0.000031   \n",
       "21                       __word__still -0.000009 -3.306890e-05 -0.000045   \n",
       "22                        __word__also -0.000039  3.451810e-05 -0.000088   \n",
       "23                         __word__don -0.000072  8.575820e-05 -0.000050   \n",
       "24                        __word__coin -0.000116 -3.001440e-04  0.000009   \n",
       "25                       __word__value -0.000078  7.626980e-05  0.000021   \n",
       "26                    __word__currency  0.000039 -5.104700e-05  0.000061   \n",
       "27                        __word__many -0.000031 -3.920700e-05 -0.000122   \n",
       "28                        __word__long  0.000058  2.467870e-05  0.000169   \n",
       "29                      __word__really -0.000065  3.799370e-07  0.000027   \n",
       "...                                ...       ...           ...       ...   \n",
       "78961                __word__fuckaling  0.000907 -2.620640e-03  0.000498   \n",
       "78962                    __word__eople -0.001287  1.048680e-03 -0.001665   \n",
       "78963                     __word__aoon  0.000510  4.815000e-04  0.000113   \n",
       "78964                 __word__stardate -0.000558 -2.062710e-03 -0.001540   \n",
       "78965                   __word__fatboy -0.001363 -9.426400e-04 -0.000525   \n",
       "78966                    __word__nabob -0.002292 -7.951020e-04 -0.001822   \n",
       "78967                      __word__bjp -0.000667 -1.633440e-03 -0.002409   \n",
       "78968  __word__eedddbabeeefadfddbcccbe -0.000426  1.016100e-04  0.001010   \n",
       "78969                    __word__cqkby  0.000231  4.224240e-05  0.000275   \n",
       "78970                     __word__huhh  0.000083 -3.858930e-04 -0.001691   \n",
       "78971             __word__significantl  0.002670 -1.905960e-03  0.000973   \n",
       "78972                    __word__viant -0.001340 -3.159160e-03  0.002478   \n",
       "78973                    __word__faull  0.000349 -1.992730e-03  0.000649   \n",
       "78974              __word__strengthing  0.001906 -1.644840e-03  0.002119   \n",
       "78975               __word__buybitcoin  0.001752  2.305100e-03 -0.000603   \n",
       "78976                    __word__netqk  0.000013  1.526940e-03 -0.000789   \n",
       "78977                 __word__jammalan -0.002422  1.973180e-03 -0.000510   \n",
       "78978               __word__touristing -0.001092  2.200600e-03 -0.000692   \n",
       "78979              __word__wxngrrjhelu -0.000054  1.573170e-03 -0.000255   \n",
       "78980                  __word__woooaaa  0.001943 -6.098240e-04  0.001454   \n",
       "78981                     __word__bles  0.000144  1.040230e-03 -0.001060   \n",
       "78982                __word__dickeater -0.001418  1.613910e-03  0.001252   \n",
       "78983                   __word__mvpmbq -0.000185  3.572710e-04 -0.000071   \n",
       "78984               __word__segwitshit  0.000544 -1.371230e-03  0.001933   \n",
       "78985           __word__pseudosolution  0.002385 -1.901810e-04  0.001685   \n",
       "78986                      __word__gmn -0.001890  5.482230e-04  0.001661   \n",
       "78987                   __word__bqrxqq -0.002729  1.984420e-04 -0.000281   \n",
       "78988           __word__nocoinersbtfoo -0.002117 -4.120080e-03 -0.000378   \n",
       "78989                    __word__wcasy -0.000874 -5.617070e-04  0.002005   \n",
       "78990                __word__dippening -0.000080 -2.211080e-03 -0.000238   \n",
       "\n",
       "             4             5         6         7             8         9   \\\n",
       "0      0.000365  8.903410e-04 -0.000867  0.000126 -8.548350e-04  0.001290   \n",
       "1     -0.000031  2.896700e-05 -0.000038  0.000014  4.506230e-07 -0.000032   \n",
       "2     -0.000084 -6.160200e-06  0.000005  0.000058 -4.459280e-05  0.000004   \n",
       "3     -0.000021 -1.690560e-05 -0.000029 -0.000035 -1.161040e-04 -0.000003   \n",
       "4     -0.000026 -3.616490e-05  0.000025 -0.000018 -2.569730e-05 -0.000017   \n",
       "5     -0.000011  7.164010e-05 -0.000066  0.000035  1.904780e-07 -0.000034   \n",
       "6     -0.000063 -3.680070e-05 -0.000003  0.000063 -9.256760e-05 -0.000058   \n",
       "7     -0.000017 -5.420950e-06 -0.000010 -0.000028 -6.218520e-05 -0.000002   \n",
       "8      0.000034  9.840050e-05  0.000023 -0.000077  2.076430e-05 -0.000013   \n",
       "9     -0.000011  2.281250e-05 -0.000005 -0.000014 -5.124530e-05  0.000012   \n",
       "10    -0.000074 -1.088330e-04  0.000011  0.000052 -1.135720e-05  0.000054   \n",
       "11    -0.000008 -5.899240e-05 -0.000035 -0.000081 -5.015670e-05  0.000019   \n",
       "12     0.000001  1.716380e-05  0.000020 -0.000018 -7.456680e-05 -0.000039   \n",
       "13     0.000038  2.103190e-05 -0.000008 -0.000026 -1.196680e-04  0.000025   \n",
       "14     0.000090 -1.015540e-04  0.000067 -0.000043 -1.142120e-04  0.000167   \n",
       "15    -0.000117 -4.460470e-05  0.000102  0.000013 -2.935990e-05 -0.000004   \n",
       "16    -0.000025  2.324080e-05  0.000023 -0.000013 -6.081730e-05 -0.000033   \n",
       "17    -0.000040 -4.551940e-05  0.000040  0.000044 -3.047290e-05 -0.000168   \n",
       "18    -0.000062 -9.969450e-07  0.000013 -0.000023 -1.119710e-05 -0.000054   \n",
       "19    -0.000110  4.661170e-05  0.000021  0.000046 -2.217640e-04  0.000019   \n",
       "20    -0.000049  2.240520e-05  0.000052 -0.000032  2.252010e-05  0.000014   \n",
       "21    -0.000173  4.552790e-05 -0.000005 -0.000037  5.556940e-05  0.000022   \n",
       "22     0.000075 -2.887120e-05 -0.000037 -0.000063 -6.824580e-05 -0.000010   \n",
       "23    -0.000084 -1.258500e-05  0.000225  0.000084 -7.119820e-05 -0.000055   \n",
       "24     0.000093 -7.908060e-05 -0.000014 -0.000017 -1.419140e-04 -0.000213   \n",
       "25     0.000135 -1.754790e-05  0.000067 -0.000001 -2.681780e-05 -0.000103   \n",
       "26     0.000103 -1.194060e-04 -0.000181 -0.000058 -1.096660e-04  0.000050   \n",
       "27    -0.000002  2.812940e-05 -0.000071 -0.000094 -1.046650e-04  0.000013   \n",
       "28    -0.000088  4.845060e-05  0.000093 -0.000030 -3.372640e-05  0.000073   \n",
       "29    -0.000165  2.802190e-05  0.000030 -0.000108 -7.411120e-06 -0.000041   \n",
       "...         ...           ...       ...       ...           ...       ...   \n",
       "78961 -0.001182 -2.168760e-03 -0.000510 -0.002193  1.836560e-03  0.002248   \n",
       "78962 -0.001390 -2.108740e-04 -0.001398  0.001089  1.231100e-03 -0.000035   \n",
       "78963 -0.000837 -1.127690e-03  0.002258  0.000324 -9.777360e-04  0.001104   \n",
       "78964 -0.001117  9.955040e-04  0.001450 -0.001055  1.824730e-03  0.002824   \n",
       "78965  0.001969 -1.551090e-04  0.001986 -0.001082  5.994700e-04 -0.001049   \n",
       "78966  0.000040  1.390690e-03  0.000987 -0.003352 -9.738830e-05  0.000800   \n",
       "78967  0.000931  5.747870e-04  0.000407 -0.000794 -1.224550e-03  0.001605   \n",
       "78968 -0.001327 -1.590300e-04 -0.001064 -0.000947 -4.533870e-04 -0.000355   \n",
       "78969 -0.001691  1.667710e-03 -0.003467  0.001803 -1.431730e-03 -0.000203   \n",
       "78970 -0.000520 -6.876020e-04 -0.002378 -0.000701 -2.563880e-03 -0.002059   \n",
       "78971  0.000531  9.084450e-04  0.001215 -0.001860  1.333330e-03  0.000463   \n",
       "78972 -0.000299  1.237350e-03  0.000834  0.000047 -1.586510e-03  0.001988   \n",
       "78973 -0.001585 -1.357520e-03 -0.000219 -0.001246 -1.767200e-03  0.001626   \n",
       "78974 -0.000566 -2.000040e-04  0.000472  0.000382  1.723080e-04 -0.002437   \n",
       "78975 -0.001633  5.395210e-04 -0.001625 -0.001842  1.156650e-03  0.000322   \n",
       "78976 -0.000559  6.881080e-04 -0.000556 -0.000924  6.732900e-04 -0.001681   \n",
       "78977 -0.000935  1.145050e-03 -0.001999  0.001057 -7.002450e-04  0.002785   \n",
       "78978 -0.001479  9.945140e-04  0.000429 -0.003104 -1.664580e-03 -0.001810   \n",
       "78979  0.000714 -2.026220e-03 -0.000138 -0.001334 -2.687830e-05  0.000177   \n",
       "78980 -0.000369  2.648760e-03 -0.000970  0.001354 -5.739110e-04 -0.000659   \n",
       "78981 -0.002453 -8.893930e-04 -0.001766  0.002186  5.628800e-05 -0.001417   \n",
       "78982  0.000707 -1.188980e-03 -0.001594 -0.001036 -4.906060e-05 -0.002260   \n",
       "78983 -0.000033 -1.749650e-03  0.000436 -0.000169  2.765820e-04  0.002027   \n",
       "78984  0.002346 -1.738810e-03 -0.002046  0.002466 -1.147520e-03 -0.002247   \n",
       "78985  0.002422 -3.860420e-04 -0.001809  0.001462 -3.688650e-03 -0.002483   \n",
       "78986 -0.000097 -1.437860e-03 -0.000999 -0.000371  4.123920e-04 -0.001373   \n",
       "78987  0.001552 -9.748660e-04  0.000185 -0.001724 -2.079110e-03 -0.001042   \n",
       "78988  0.001199 -1.104440e-05 -0.001101  0.000285 -6.928000e-04 -0.000447   \n",
       "78989  0.001123 -1.120120e-04 -0.001738 -0.000820  1.784230e-03 -0.003275   \n",
       "78990 -0.001184 -8.126880e-04  0.000314 -0.004313  6.895810e-04  0.000910   \n",
       "\n",
       "           ...             41        42        43        44        45  \\\n",
       "0          ...      -0.000868  0.001118 -0.002038 -0.001416 -0.001188   \n",
       "1          ...      -0.000008 -0.000016 -0.000029  0.000015 -0.000010   \n",
       "2          ...       0.000023  0.000007 -0.000041 -0.000026  0.000058   \n",
       "3          ...       0.000027 -0.000037  0.000019 -0.000025  0.000066   \n",
       "4          ...      -0.000020 -0.000019  0.000005  0.000014  0.000015   \n",
       "5          ...       0.000007 -0.000003 -0.000060  0.000041  0.000021   \n",
       "6          ...      -0.000046 -0.000039  0.000052 -0.000032 -0.000095   \n",
       "7          ...       0.000003  0.000041 -0.000017 -0.000033  0.000029   \n",
       "8          ...      -0.000104  0.000027 -0.000029  0.000065  0.000015   \n",
       "9          ...       0.000015  0.000055 -0.000024  0.000062  0.000034   \n",
       "10         ...      -0.000022 -0.000116 -0.000045  0.000032 -0.000027   \n",
       "11         ...      -0.000017 -0.000022 -0.000016  0.000015  0.000015   \n",
       "12         ...       0.000013  0.000090 -0.000055  0.000036  0.000025   \n",
       "13         ...       0.000058  0.000046 -0.000017 -0.000021  0.000014   \n",
       "14         ...      -0.000056 -0.000143 -0.000014 -0.000023  0.000016   \n",
       "15         ...       0.000030  0.000024 -0.000043 -0.000072  0.000014   \n",
       "16         ...       0.000007 -0.000018  0.000006 -0.000023 -0.000019   \n",
       "17         ...       0.000009 -0.000079  0.000041  0.000018 -0.000085   \n",
       "18         ...      -0.000042 -0.000091 -0.000074 -0.000040  0.000048   \n",
       "19         ...       0.000127  0.000204  0.000014  0.000213 -0.000087   \n",
       "20         ...      -0.000043  0.000038 -0.000024  0.000019  0.000015   \n",
       "21         ...      -0.000132  0.000042  0.000033  0.000040 -0.000026   \n",
       "22         ...       0.000054  0.000029 -0.000027 -0.000030  0.000046   \n",
       "23         ...      -0.000035  0.000058  0.000005  0.000043  0.000010   \n",
       "24         ...       0.000097 -0.000078  0.000076 -0.000015 -0.000086   \n",
       "25         ...       0.000006  0.000008 -0.000229 -0.000058  0.000141   \n",
       "26         ...      -0.000219 -0.000111 -0.000203 -0.000264  0.000147   \n",
       "27         ...       0.000029 -0.000038 -0.000017  0.000002  0.000008   \n",
       "28         ...      -0.000041 -0.000030  0.000039  0.000075  0.000043   \n",
       "29         ...       0.000047  0.000076 -0.000141 -0.000018  0.000027   \n",
       "...        ...            ...       ...       ...       ...       ...   \n",
       "78961      ...       0.001063  0.001047 -0.002676  0.002291 -0.001325   \n",
       "78962      ...      -0.000432  0.001909 -0.001453 -0.002717 -0.003381   \n",
       "78963      ...       0.000950 -0.001155 -0.000083 -0.000600 -0.000359   \n",
       "78964      ...      -0.001912 -0.000733 -0.000808  0.002837  0.002194   \n",
       "78965      ...       0.000813  0.000703  0.001040  0.000856  0.002484   \n",
       "78966      ...       0.000625 -0.001012  0.001441  0.000958  0.000333   \n",
       "78967      ...       0.000386 -0.000648  0.001867  0.003074  0.002686   \n",
       "78968      ...      -0.001863  0.000017 -0.001914  0.002784 -0.002119   \n",
       "78969      ...       0.000938 -0.003094 -0.002979  0.000290 -0.000513   \n",
       "78970      ...       0.001561  0.000033  0.000586 -0.000801  0.001175   \n",
       "78971      ...       0.000028  0.000361 -0.000452  0.001392  0.000973   \n",
       "78972      ...      -0.001355  0.001303 -0.004386 -0.000741 -0.000873   \n",
       "78973      ...      -0.001054 -0.001224 -0.002171  0.001704 -0.000417   \n",
       "78974      ...      -0.002486 -0.001053  0.001422  0.000872  0.001521   \n",
       "78975      ...       0.000562 -0.000322 -0.003280  0.002624  0.000044   \n",
       "78976      ...       0.001229 -0.000499 -0.000255 -0.000138  0.001192   \n",
       "78977      ...       0.000937 -0.001023 -0.002352 -0.000746 -0.000004   \n",
       "78978      ...       0.000604 -0.000505 -0.001069 -0.000977  0.000939   \n",
       "78979      ...      -0.000774  0.000895  0.000424 -0.001245 -0.001289   \n",
       "78980      ...      -0.002499 -0.001729  0.001013 -0.002637  0.000772   \n",
       "78981      ...       0.000260 -0.000540  0.000656 -0.001577 -0.003603   \n",
       "78982      ...      -0.001401 -0.001132 -0.002322  0.002753  0.002409   \n",
       "78983      ...      -0.000351 -0.001194 -0.002549  0.000951 -0.004459   \n",
       "78984      ...      -0.000538 -0.002259  0.000246  0.001422 -0.002065   \n",
       "78985      ...      -0.000816 -0.002545  0.000971  0.003027 -0.001891   \n",
       "78986      ...       0.004555  0.000049  0.000006  0.002189 -0.002355   \n",
       "78987      ...       0.000048 -0.002144  0.000450 -0.001110 -0.002782   \n",
       "78988      ...      -0.000816 -0.000147  0.000659  0.001360  0.000485   \n",
       "78989      ...      -0.000115 -0.001132 -0.002303 -0.000020 -0.001237   \n",
       "78990      ...       0.001923  0.001714 -0.000897  0.001333 -0.001341   \n",
       "\n",
       "             46            47            48        49            50  \n",
       "0      0.000272 -1.039700e-03  1.249780e-03 -0.000796 -8.301510e-05  \n",
       "1     -0.000019 -4.357470e-05  2.011180e-05  0.000009 -7.639540e-06  \n",
       "2     -0.000013 -1.563940e-05  1.153700e-05 -0.000031 -2.430360e-06  \n",
       "3      0.000021  1.222220e-05 -5.887540e-05  0.000007  3.758550e-05  \n",
       "4     -0.000032 -3.797110e-05 -1.402450e-05  0.000012 -6.772960e-06  \n",
       "5     -0.000062 -8.350000e-05  6.057710e-05  0.000056 -4.780960e-05  \n",
       "6     -0.000008 -6.154720e-06 -5.206500e-06  0.000013 -1.546800e-05  \n",
       "7     -0.000018 -3.421030e-05  4.295350e-05 -0.000036  1.250570e-07  \n",
       "8     -0.000033  2.274020e-05  8.231730e-05  0.000019 -1.743520e-06  \n",
       "9     -0.000017  4.706920e-06 -5.948090e-05 -0.000021 -5.747050e-05  \n",
       "10    -0.000004 -9.676540e-05  8.301950e-06  0.000038  1.810370e-05  \n",
       "11     0.000035  4.682290e-05  5.608910e-05 -0.000106  8.471740e-05  \n",
       "12    -0.000009  1.202550e-05  1.455940e-05 -0.000014 -8.176780e-05  \n",
       "13    -0.000033  5.423480e-05 -3.767350e-05 -0.000082  2.584870e-05  \n",
       "14     0.000142  1.277130e-04  6.132710e-05  0.000027  1.761470e-04  \n",
       "15     0.000031 -1.164980e-07 -6.795670e-05 -0.000039 -5.392040e-05  \n",
       "16     0.000150  4.326830e-05  4.024570e-05 -0.000028 -2.255610e-05  \n",
       "17    -0.000072 -2.725490e-05  6.437440e-05 -0.000059  5.761320e-05  \n",
       "18     0.000016 -6.702450e-05  3.389700e-05  0.000006  6.228510e-05  \n",
       "19    -0.000031  1.007680e-04  4.726460e-05  0.000349 -1.575370e-04  \n",
       "20     0.000035 -3.407370e-05  1.734660e-06 -0.000014 -8.174900e-06  \n",
       "21     0.000033 -4.815830e-05  4.127900e-06  0.000054 -9.324580e-05  \n",
       "22    -0.000024 -2.972300e-06  4.834600e-05  0.000028  5.954240e-05  \n",
       "23    -0.000001  5.082580e-06 -1.365750e-04 -0.000039 -4.572480e-06  \n",
       "24     0.000115 -1.975670e-04  5.710110e-05 -0.000120  5.518170e-05  \n",
       "25    -0.000018  3.797800e-05  9.211330e-08  0.000021 -2.054590e-05  \n",
       "26     0.000103  2.212090e-06  2.685890e-05  0.000101 -6.131520e-05  \n",
       "27     0.000070 -3.293670e-05 -3.160790e-05  0.000035  5.073330e-05  \n",
       "28    -0.000081 -1.465590e-04 -6.357790e-05 -0.000071 -1.378800e-05  \n",
       "29    -0.000045  1.114940e-04 -5.830420e-05 -0.000160 -4.967930e-05  \n",
       "...         ...           ...           ...       ...           ...  \n",
       "78961  0.000567 -7.991940e-05  6.953040e-04  0.000690  1.005080e-04  \n",
       "78962 -0.001546  2.753840e-04 -1.608610e-04  0.001295 -1.380220e-03  \n",
       "78963  0.000705  1.474880e-03  1.524700e-03 -0.000450 -1.085590e-03  \n",
       "78964  0.003074  3.532140e-05 -1.245350e-03 -0.000844 -1.443350e-03  \n",
       "78965  0.004016 -3.046230e-05 -2.057460e-03 -0.000630 -2.503440e-03  \n",
       "78966  0.002915  6.809090e-04 -2.607660e-03  0.000948 -5.024280e-03  \n",
       "78967  0.002670  6.179370e-04 -2.472540e-03  0.002468 -3.443540e-03  \n",
       "78968  0.000304  1.732900e-03 -2.225020e-03 -0.000647 -1.470310e-03  \n",
       "78969  0.000729 -4.302520e-04 -1.313020e-04  0.000474  7.231630e-04  \n",
       "78970 -0.001148 -1.221400e-03 -2.093840e-04 -0.004070 -1.077560e-03  \n",
       "78971 -0.000217  9.564120e-04  1.875080e-03 -0.001768 -2.778280e-03  \n",
       "78972 -0.000412 -1.079340e-03 -1.037770e-03 -0.002255 -1.567260e-03  \n",
       "78973  0.002698 -9.265140e-04 -2.299090e-04 -0.001519 -7.929800e-04  \n",
       "78974  0.000900  2.477130e-03 -5.135250e-05 -0.000675 -1.663800e-03  \n",
       "78975  0.000934 -2.825110e-04  1.912130e-04 -0.001442  1.166860e-03  \n",
       "78976  0.000239 -1.363180e-03 -1.091070e-03  0.001180  2.802170e-05  \n",
       "78977 -0.002133 -1.206440e-03 -2.821130e-04 -0.001127 -1.360350e-04  \n",
       "78978  0.000814 -2.789180e-03 -1.972160e-03 -0.001505 -5.836550e-04  \n",
       "78979 -0.000539 -1.497370e-03 -1.036310e-03 -0.001056 -9.411020e-04  \n",
       "78980 -0.001054  9.044390e-04 -2.354210e-04 -0.001018 -3.219100e-04  \n",
       "78981  0.000579 -1.140140e-03  1.910580e-03  0.000487  5.976540e-04  \n",
       "78982 -0.001146 -3.656990e-04 -4.297690e-04 -0.001808  4.614080e-04  \n",
       "78983 -0.002380  2.801240e-03  2.349990e-05  0.000378 -2.223390e-03  \n",
       "78984  0.002164 -1.401510e-04  2.481260e-03 -0.000318 -1.579690e-03  \n",
       "78985  0.002263 -2.271970e-03  1.656410e-03 -0.001163 -2.956470e-03  \n",
       "78986 -0.000678  2.456270e-03 -1.055290e-03 -0.000333  4.490640e-04  \n",
       "78987  0.000808 -1.376350e-03  5.834340e-04  0.000033  2.126130e-03  \n",
       "78988 -0.001595 -6.368150e-04 -3.036520e-04  0.000391  4.633280e-03  \n",
       "78989 -0.000496  8.010320e-04 -1.125800e-03  0.000569 -2.823520e-03  \n",
       "78990  0.001248 -2.494900e-04  1.547960e-03  0.001972  7.585590e-05  \n",
       "\n",
       "[78991 rows x 51 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_space_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = word_space_df.iloc[:,1:].values*100\n",
    "# np.savetxt('embedding_matrix.txt', embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.loadtxt('embedding_matrix.txt', dtype=float)\n",
    "embedding_matrix == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.91330e-02,  2.30685e-02, -2.48654e-01, ...,  1.24978e-01,\n",
       "        -7.95619e-02, -8.30151e-03],\n",
       "       [ 5.85944e-04,  2.97384e-04,  3.66383e-04, ...,  2.01118e-03,\n",
       "         8.94584e-04, -7.63954e-04],\n",
       "       [ 2.64723e-03, -4.01565e-04,  5.92613e-04, ...,  1.15370e-03,\n",
       "        -3.05743e-03, -2.43036e-04],\n",
       "       ...,\n",
       "       [-2.11684e-01, -4.12008e-01, -3.77738e-02, ..., -3.03652e-02,\n",
       "         3.91285e-02,  4.63328e-01],\n",
       "       [-8.74419e-02, -5.61707e-02,  2.00460e-01, ..., -1.12580e-01,\n",
       "         5.69454e-02, -2.82352e-01],\n",
       "       [-7.97337e-03, -2.21108e-01, -2.37592e-02, ...,  1.54796e-01,\n",
       "         1.97180e-01,  7.58559e-03]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78991"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_ready_all_data_word_to_vec.csv', parse_dates = (['timestamp'])).set_index('timestamp')\n",
    "df.sort_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_vec_data = df['word_to_vec_id'].values\n",
    "price_label = df['price_change'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "word_to_vec_data = [json.loads(elem) for elem in word_to_vec_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_label = price_label*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_label = [-1 if elem <= -1 else elem for elem in price_label ]\n",
    "price_label = [1 if elem >= 1 else elem for elem in price_label ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([89868., 11345., 14080., 18875., 32849., 37072., 22085., 16630.,\n",
       "        13124., 95738.]),\n",
       " array([-1. , -0.8, -0.6, -0.4, -0.2,  0. ,  0.2,  0.4,  0.6,  0.8,  1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5a22dd4f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(price_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   11   803    58 ... 78991 78991 78991]\n",
      " [28584  3782  1226 ... 78991 78991 78991]\n",
      " [ 1252    66   651 ... 78991 78991 78991]\n",
      " ...\n",
      " [  315  3087     5 ... 78991 78991 78991]\n",
      " [  292    12    27 ... 78991 78991 78991]\n",
      " [  403    66     1 ... 78991 78991 78991]]\n"
     ]
    }
   ],
   "source": [
    "emb_len = len( np.loadtxt('embedding_matrix.txt', dtype=float))\n",
    "padded_docs = pad_sequences(word_to_vec_data, maxlen=150, padding='post', value=emb_len)\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide to train test validation sets (80% 10% 10%)\n",
    "additional_data = df[['polarity', 'subjectivity']].values\n",
    "\n",
    "X_train, X_test, y_train, y_test, additional_train, additional_test = train_test_split(padded_docs, price_label, additional_data, test_size=0.1, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val, additional_train, additional_val = train_test_split(X_train, y_train, additional_train, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = [len(elem) for elem in padded_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([     0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0., 351666.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.]),\n",
       " array([149.5 , 149.51, 149.52, 149.53, 149.54, 149.55, 149.56, 149.57,\n",
       "        149.58, 149.59, 149.6 , 149.61, 149.62, 149.63, 149.64, 149.65,\n",
       "        149.66, 149.67, 149.68, 149.69, 149.7 , 149.71, 149.72, 149.73,\n",
       "        149.74, 149.75, 149.76, 149.77, 149.78, 149.79, 149.8 , 149.81,\n",
       "        149.82, 149.83, 149.84, 149.85, 149.86, 149.87, 149.88, 149.89,\n",
       "        149.9 , 149.91, 149.92, 149.93, 149.94, 149.95, 149.96, 149.97,\n",
       "        149.98, 149.99, 150.  , 150.01, 150.02, 150.03, 150.04, 150.05,\n",
       "        150.06, 150.07, 150.08, 150.09, 150.1 , 150.11, 150.12, 150.13,\n",
       "        150.14, 150.15, 150.16, 150.17, 150.18, 150.19, 150.2 , 150.21,\n",
       "        150.22, 150.23, 150.24, 150.25, 150.26, 150.27, 150.28, 150.29,\n",
       "        150.3 , 150.31, 150.32, 150.33, 150.34, 150.35, 150.36, 150.37,\n",
       "        150.38, 150.39, 150.4 , 150.41, 150.42, 150.43, 150.44, 150.45,\n",
       "        150.46, 150.47, 150.48, 150.49, 150.5 ]),\n",
       " <a list of 100 Patch objects>)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAFpFJREFUeJzt3X+snmWd5/H3x1YYZmelVQ4sQ8mW0W5GJLFiF7sx2XVhFgp/bHED2fKHNKamLgu7M5uZXXEmGRyVRDNREibKBkOHYhyRRV0ardNtEOOaKHDU8qMi0zPgSKVDiy2IMeKC3/3juaoP5ek5V8+PPsW+X8md576/93Vf93UBPZ/eP55DqgpJknq8atwDkCS9chgakqRuhoYkqZuhIUnqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6LR73AObbKaecUsuXLx/3MCTpFeXb3/7201U1MVO737jQWL58OZOTk+MehiS9oiT5h5523p6SJHUzNCRJ3QwNSVI3Q0OS1M3QkCR1MzQkSd1mDI0kv5XkviQPJNmZ5C9a/dYkjyfZ0ZaVrZ4kNyaZSvJgknOH+lqfZFdb1g/V35rkoXbMjUnS6q9Nsr21355k6fz/I5Ak9eq50ngeOL+q3gysBNYkWd32/feqWtmWHa12MbCiLRuBm2AQAMB1wNuA84DrhkLgptb24HFrWv1a4O6qWgHc3bYlSWMyY2jUwE/b5qvbMt3/WHwtcFs77lvAkiSnAxcB26tqf1UdALYzCKDTgddU1Tdr8D8svw24dKivzW1981BdkjQGXc80kixKsgPYy+AH/71t1/XtFtQNSU5stTOAJ4YO391q09V3j6gDnFZVewDa56ndM5OOMcuv/fKvFumVqis0qurFqloJLAPOS3IO8H7g94F/CbwWeF9rnlFdzKLeLcnGJJNJJvft23ckh0qSjsARvT1VVc8AXwPWVNWedgvqeeCvGTyngMGVwplDhy0DnpyhvmxEHeCpdvuK9rn3MOO6uapWVdWqiYkZf9+WJGmWet6emkiypK2fBPwB8P2hH+Zh8Kzh4XbIFuDK9hbVauDZdmtpG3BhkqXtAfiFwLa277kkq1tfVwJ3DfV18C2r9UN1SdIY9PyW29OBzUkWMQiZO6rqS0m+mmSCwe2lHcB/au23ApcAU8DPgHcDVNX+JB8C7m/tPlhV+9v6VcCtwEnAV9oC8BHgjiQbgB8Cl892opKkuZsxNKrqQeAtI+rnH6Z9AVcfZt8mYNOI+iRwzoj6j4ELZhqjJOno8BvhkqRuhoYkqZuhIUnqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6zRgaSX4ryX1JHkiyM8lftPpZSe5NsivJ55Kc0Oontu2ptn/5UF/vb/VHk1w0VF/TalNJrh2qjzyHJGk8eq40ngfOr6o3AyuBNUlWAx8FbqiqFcABYENrvwE4UFVvAG5o7UhyNrAOeBOwBvhkkkVJFgGfAC4GzgauaG2Z5hySpDGYMTRq4Kdt89VtKeB84M5W3wxc2tbXtm3a/guSpNVvr6rnq+pxYAo4ry1TVfVYVf0CuB1Y24453DkkSWPQ9UyjXRHsAPYC24G/B56pqhdak93AGW39DOAJgLb/WeB1w/VDjjlc/XXTnEOSNAZdoVFVL1bVSmAZgyuDN45q1j5zmH3zVX+ZJBuTTCaZ3Ldv36gmkqR5cERvT1XVM8DXgNXAkiSL265lwJNtfTdwJkDbfzKwf7h+yDGHqz89zTkOHdfNVbWqqlZNTEwcyZQkSUeg5+2piSRL2vpJwB8AjwD3AJe1ZuuBu9r6lrZN2//VqqpWX9ferjoLWAHcB9wPrGhvSp3A4GH5lnbM4c4hSRqDxTM34XRgc3vL6VXAHVX1pSTfA25P8mHgu8Atrf0twKeTTDG4wlgHUFU7k9wBfA94Abi6ql4ESHINsA1YBGyqqp2tr/cd5hySpDGYMTSq6kHgLSPqjzF4vnFo/efA5Yfp63rg+hH1rcDW3nNIksbDb4RLkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSpm6EhSepmaEiSus0YGknOTHJPkkeS7Ezyh63+gSQ/SrKjLZcMHfP+JFNJHk1y0VB9TatNJbl2qH5WknuT7EryuSQntPqJbXuq7V8+n5OXJB2ZniuNF4A/rqo3AquBq5Oc3fbdUFUr27IVoO1bB7wJWAN8MsmiJIuATwAXA2cDVwz189HW1wrgALCh1TcAB6rqDcANrZ0kaUxmDI2q2lNV32nrzwGPAGdMc8ha4Paqer6qHgemgPPaMlVVj1XVL4DbgbVJApwP3NmO3wxcOtTX5rZ+J3BBay9JGoMjeqbRbg+9Bbi3la5J8mCSTUmWttoZwBNDh+1utcPVXwc8U1UvHFJ/SV9t/7OtvSRpDLpDI8nvAJ8H/qiqfgLcBLweWAnsAT52sOmIw2sW9en6OnRsG5NMJpnct2/ftPOQJM1eV2gkeTWDwPhMVX0BoKqeqqoXq+qXwKcY3H6CwZXCmUOHLwOenKb+NLAkyeJD6i/pq+0/Gdh/6Piq6uaqWlVVqyYmJnqmJEmahZ63pwLcAjxSVR8fqp8+1OydwMNtfQuwrr35dBawArgPuB9Y0d6UOoHBw/ItVVXAPcBl7fj1wF1Dfa1v65cBX23tJUljsHjmJrwdeBfwUJIdrfanDN5+WsngdtEPgPcCVNXOJHcA32Pw5tXVVfUiQJJrgG3AImBTVe1s/b0PuD3Jh4HvMggp2uenk0wxuMJYN4e5SpLmaMbQqKpvMPrZwtZpjrkeuH5Efeuo46rqMX59e2u4/nPg8pnGKEk6OvxGuCSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnqZmhIkroZGpKkbjOGRpIzk9yT5JEkO5P8Yau/Nsn2JLva59JWT5Ibk0wleTDJuUN9rW/tdyVZP1R/a5KH2jE3Jsl055AkjUfPlcYLwB9X1RuB1cDVSc4GrgXurqoVwN1tG+BiYEVbNgI3wSAAgOuAtwHnAdcNhcBNre3B49a0+uHOIUkagxlDo6r2VNV32vpzwCPAGcBaYHNrthm4tK2vBW6rgW8BS5KcDlwEbK+q/VV1ANgOrGn7XlNV36yqAm47pK9R55AkjcERPdNIshx4C3AvcFpV7YFBsACntmZnAE8MHba71aar7x5RZ5pzSJLGoDs0kvwO8Hngj6rqJ9M1HVGrWdS7JdmYZDLJ5L59+47kUEnSEegKjSSvZhAYn6mqL7TyU+3WEu1zb6vvBs4cOnwZ8OQM9WUj6tOd4yWq6uaqWlVVqyYmJnqmJEmahZ63pwLcAjxSVR8f2rUFOPgG1HrgrqH6le0tqtXAs+3W0jbgwiRL2wPwC4Ftbd9zSVa3c115SF+jziFJGoPFHW3eDrwLeCjJjlb7U+AjwB1JNgA/BC5v+7YClwBTwM+AdwNU1f4kHwLub+0+WFX72/pVwK3AScBX2sI055AkjcGMoVFV32D0cweAC0a0L+Dqw/S1Cdg0oj4JnDOi/uNR55AkjYffCJckdTM0JEndDA1JUjdDQ5LUzdCQJHUzNCRJ3QwNSVI3Q0OS1M3QkCR1MzQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUzdCQJHUzNCRJ3QwNSVI3Q0OS1M3QkCR1mzE0kmxKsjfJw0O1DyT5UZIdbblkaN/7k0wleTTJRUP1Na02leTaofpZSe5NsivJ55Kc0Oontu2ptn/5fE1akjQ7PVcatwJrRtRvqKqVbdkKkORsYB3wpnbMJ5MsSrII+ARwMXA2cEVrC/DR1tcK4ACwodU3AAeq6g3ADa2dJGmMZgyNqvo6sL+zv7XA7VX1fFU9DkwB57Vlqqoeq6pfALcDa5MEOB+4sx2/Gbh0qK/Nbf1O4ILWXpI0JnN5pnFNkgfb7aulrXYG8MRQm92tdrj664BnquqFQ+ov6avtf7a1f5kkG5NMJpnct2/fHKYkSZrObEPjJuD1wEpgD/CxVh91JVCzqE/X18uLVTdX1aqqWjUxMTHduCVJczCr0Kiqp6rqxar6JfApBrefYHClcOZQ02XAk9PUnwaWJFl8SP0lfbX9J9N/m0yStABmFRpJTh/afCdw8M2qLcC69ubTWcAK4D7gfmBFe1PqBAYPy7dUVQH3AJe149cDdw31tb6tXwZ8tbWXJI3J4pkaJPks8A7glCS7geuAdyRZyeB20Q+A9wJU1c4kdwDfA14Arq6qF1s/1wDbgEXApqra2U7xPuD2JB8Gvgvc0uq3AJ9OMsXgCmPdnGcrSZqTGUOjqq4YUb5lRO1g++uB60fUtwJbR9Qf49e3t4brPwcun2l8kqSjx2+ES5K6GRqSpG6GhiSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnqNmNoJNmUZG+Sh4dqr02yPcmu9rm01ZPkxiRTSR5Mcu7QMetb+11J1g/V35rkoXbMjUky3TkkSePTc6VxK7DmkNq1wN1VtQK4u20DXAysaMtG4CYYBABwHfA24DzguqEQuKm1PXjcmhnOIUkakxlDo6q+Duw/pLwW2NzWNwOXDtVvq4FvAUuSnA5cBGyvqv1VdQDYDqxp+15TVd+sqgJuO6SvUeeQJI3JbJ9pnFZVewDa56mtfgbwxFC73a02XX33iPp055Akjcl8PwjPiFrNon5kJ002JplMMrlv374jPVyS1Gm2ofFUu7VE+9zb6ruBM4faLQOenKG+bER9unO8TFXdXFWrqmrVxMTELKckSZrJbENjC3DwDaj1wF1D9SvbW1SrgWfbraVtwIVJlrYH4BcC29q+55Ksbm9NXXlIX6POIUkak8UzNUjyWeAdwClJdjN4C+ojwB1JNgA/BC5vzbcClwBTwM+AdwNU1f4kHwLub+0+WFUHH65fxeANrZOAr7SFac4hSRqTGUOjqq44zK4LRrQt4OrD9LMJ2DSiPgmcM6L+41HnkCSNj98IlyR1MzQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUzdCQJHUzNCRJ3QwNSVI3Q0OS1M3QkCR1MzQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUzdCQJHWbU2gk+UGSh5LsSDLZaq9Nsj3Jrva5tNWT5MYkU0keTHLuUD/rW/tdSdYP1d/a+p9qx2Yu45Ukzc18XGn826paWVWr2va1wN1VtQK4u20DXAysaMtG4CYYhAxwHfA24DzguoNB09psHDpuzTyMV5I0Swtxe2otsLmtbwYuHarfVgPfApYkOR24CNheVfur6gCwHVjT9r2mqr5ZVQXcNtSXJGkM5hoaBfyfJN9OsrHVTquqPQDt89RWPwN4YujY3a02XX33iPrLJNmYZDLJ5L59++Y4JUnS4Sye4/Fvr6onk5wKbE/y/WnajnoeUbOov7xYdTNwM8CqVatGtpEkzd2crjSq6sn2uRf4IoNnEk+1W0u0z72t+W7gzKHDlwFPzlBfNqIuSRqTWYdGkn+S5J8eXAcuBB4GtgAH34BaD9zV1rcAV7a3qFYDz7bbV9uAC5MsbQ/ALwS2tX3PJVnd3pq6cqgvSdIYzOX21GnAF9tbsIuBv6mqv01yP3BHkg3AD4HLW/utwCXAFPAz4N0AVbU/yYeA+1u7D1bV/rZ+FXArcBLwlbZIksZk1qFRVY8Bbx5R/zFwwYh6AVcfpq9NwKYR9UngnNmOUZI0v/xGuCSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnqZmhIkroZGpKkbsd8aCRZk+TRJFNJrh33eCTpeHZMh0aSRcAngIuBs4Erkpw93lFJ0vHrmA4N4Dxgqqoeq6pfALcDa8c8Jkk6bh3roXEG8MTQ9u5WkySNweJxD2AGGVGrlzVKNgIb2+ZPkzy6oKNaGKcAT497EEfR8TZfGJpzPjrmkRw9x/W/51eYf97T6FgPjd3AmUPby4AnD21UVTcDNx+tQS2EJJNVtWrc4zhajrf5gnM+Xvymz/lYvz11P7AiyVlJTgDWAVvGPCZJOm4d01caVfVCkmuAbcAiYFNV7RzzsCTpuHVMhwZAVW0Fto57HEfBK/r22iwcb/MF53y8+I2ec6pe9lxZkqSRjvVnGpKkY4ihMc+SbEqyN8nDI/b9SZJKckrbXprki0keTHJfknMO02eSXJ/k75I8kuS/LvQ8jsQCzfmCJN9JsiPJN5K8YaHncSRGzTnJB5L8qI15R5JLhva9v/0qnEeTXHSYPs9Kcm+SXUk+117+OGYs0Jw/0/Y/3Pp/9dGYS6+FmPNQ279K8tOFHP+CqCqXeVyAfw2cCzx8SP1MBg/0/wE4pdX+Eriurf8+cPdh+nw3cBvwqrZ96rjneRTm/HfAG9v6fwZuHfc8Z5oz8AHgT0a0PRt4ADgROAv4e2DRiHZ3AOva+v8Erhr3PI/CnC9h8H2sAJ89Hubc2q4CPg38dNxzPNLFK415VlVfB/aP2HUD8D946ZcTzwbubsd9H1ie5LQRx14FfLCqftna7p3XQc/RAs25gNe09ZMZ8f2ccZpmzqOsBW6vquer6nFgisGvyPmVJAHOB+5spc3ApfM03Hkx33NufW6tBriPwXexjhkLMef2O/X+ksGfjVccQ+MoSPLvgR9V1QOH7HoA+A+tzXkMvpE56g/N64H/mGQyyVeSrFjQAc+DeZjze4CtSXYD7wI+soDDnU/XtFtvm5IsbbWeX4fzOuCZqnphmjbHqtnO+Vfabal3AX+7cMOcV3OZ8zXAlqras9CDXAiGxgJL8tvAnwF/PmL3R4ClSXYA/wX4LvDCiHYnAj+vwbdMPwVsWqDhzot5mvN/Ay6pqmXAXwMfX6DhzqebGAT8SmAP8LFW7/l1OF2/MucYNJc5D/sk8PWq+r/zO7wFMes5J/ld4HLgrxZygAvpmP+exm+A1zO4v/nA4A4Ey4DvJDmvqv6RwfOKg7cnHm/LoXYDn2/rX2TwQ/RYNqc5J5kA3lxV97bS53gF/A20qp46uJ7kU8CX2mbPr8N5GliSZHG72hj5K3OONXOc88HjrgMmgPcu0DDn1Rzn/BbgDcBU+7Px20mmquqYetFjOl5pLLCqeqiqTq2q5VW1nMF/WOdW1T8mWTL0hsx7GPxN6ycjuvnfDO53A/wbBg+Jj1nzMOcDwMlJ/kXb/nfAI0dl8HOQ5PShzXcCB9+42QKsS3JikrOAFQzu3/9Ku6d/D3BZK60H7lrYEc/dXObcjn8PcBFwxcFndse6Of57/nJV/bOhPxs/eyUFBuDbU/O9MHgDZA/w/xj8sNxwyP4f8Os3if4VsAv4PvAFYOlQu63A77b1JcCXgYeAbzL4W/jY57rAc35nm+8DwNeA3xv3PGeaM4O3YR4CHmTwA+T0ofZ/xuBtmkeBiw8z599j8ENmCvhfwInjnudRmPMLrc2Otvz5uOe50HM+pP9X3NtTfiNcktTN21OSpG6GhiSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnqZmhIkrr9fxAQCEL5HDzIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f59ff0eea20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(ll, bins = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_space_df[0] = [ww[8:] if type(ww) != float else str(ww) for ww in word_space_df[0] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_dict = word_space_df[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_dict = dict(zip(words_dict.values(), words_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json \n",
    "# with open('word_dict.json', 'w') as outfile:\n",
    "#     json.dump(words_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_dict.json') as json_data:\n",
    "    words_dict_test = json.load(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation, Embedding, Flatten, Dropout, TimeDistributed, Reshape, Lambda\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.loadtxt('embedding_matrix.txt', dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_padding_embedding = (np.zeros(len(embedding_matrix[0]), dtype=float))\n",
    "embedding_matrix = np.row_stack([embedding_matrix, zero_padding_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 8 #25\n",
    "use_dropout=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Dense, concatenate\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_input = Input(shape=(100,), dtype='int32', name='main_input')\n",
    "\n",
    "# This embedding layer will encode the input sequence\n",
    "# into a sequence of dense 512-dimensional vectors.\n",
    "# x = Embedding(output_dim=512, input_dim=10000, input_length=100)(main_input)\n",
    "\n",
    "main_input = Input(shape=(150,), dtype='int32', name='main_input')\n",
    "x = Embedding(output_dim = len(embedding_matrix[0]), input_dim = len(embedding_matrix),\n",
    "                    weights=[embedding_matrix], input_length = 150, trainable=False)(main_input)\n",
    "# A LSTM will transform the vector sequence into a single vector,\n",
    "# containing information about the entire sequence\n",
    "lstm_out = LSTM(units = 32)(x)\n",
    "\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "auxiliary_output = Dense(1, activation='sigmoid', name='aux_output')(lstm_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxiliary_input = Input(shape=(2,), name='aux_input')\n",
    "x = concatenate([lstm_out, auxiliary_input])\n",
    "\n",
    "# We stack a deep densely-connected network on top\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "\n",
    "# And finally we add the main logistic regression layer\n",
    "main_output = Dense(1, activation='tanh', name='main_output')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional_data = df[['polarity', 'subjectivity']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284849"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284849"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.array(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([np.array(X_train), additional_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "351666"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(additional_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "284849/284849 [==============================] - 576s 2ms/step - loss: 0.5842\n",
      "Epoch 2/10\n",
      "284849/284849 [==============================] - 495s 2ms/step - loss: 0.5841\n",
      "Epoch 3/10\n",
      " 48768/284849 [====>.........................] - ETA: 8:45 - loss: 0.5873"
     ]
    }
   ],
   "source": [
    "model.fit([np.array(X_train), additional_train], [np.array(y_train)],\n",
    "          epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.58^2  0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Embedding(len(embedding_matrix), len(embedding_matrix[0]), \n",
    "#                     weights=[embedding_matrix], input_length= 1, trainable=False))\n",
    "\n",
    "# model.add(LSTM(hidden_size, return_sequences=True))\n",
    "# model.add(LSTM(hidden_size, return_sequences=True))\n",
    "# if use_dropout:\n",
    "#     model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(8))\n",
    "# model.add(Dense(4))\n",
    "# model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = Adam()\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_matrix = np.loadtxt('embedding_matrix.txt', dtype=float)\n",
    "# # hidden_size = 500\n",
    "# hidden_size = 8 #25\n",
    "# use_dropout=True\n",
    "\n",
    "# #model.add(Embedding(vocabulary, hidden_size, input_length=num_steps))\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(len(embedding_matrix), len(embedding_matrix[0]), weights=[embedding_matrix], input_length= 1, trainable=False))\n",
    "\n",
    "# model.add(LSTM(hidden_size, return_sequences=True))\n",
    "# model.add(LSTM(hidden_size, return_sequences=True))\n",
    "# if use_dropout:\n",
    "#     model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(8, activation='relu'))\n",
    "# model.add(Dense(4, activation='relu'))\n",
    "# model.add(Dense(1, activation='relu'))\n",
    "# model.compile(optimizer='rmsprop', loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(x_train, y_train, batch_size=16, epochs=10)\n",
    "# score = model.evaluate(x_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one epoch = one forward pass and one backward pass of all the training examples\n",
    "batch size = the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need.\n",
    "number of iterations = number of passes, each pass using [batch size] number of examples. To be clear, one pass = one forward pass + one backward pass (we do not count the forward pass and backward pass as two different passes).\n",
    "Example: if you have 1000 training examples, and your batch size is 500, then it will take 2 iterations to complete 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 150, 50)           3949550   \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 50)                20200     \n",
      "=================================================================\n",
      "Total params: 3,969,750\n",
      "Trainable params: 20,200\n",
      "Non-trainable params: 3,949,550\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 150, 50)           3949550   \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 3,969,801\n",
      "Trainable params: 20,251\n",
      "Non-trainable params: 3,949,550\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.loadtxt('embedding_matrix.txt', dtype=float)\n",
    "# hidden_size = 500\n",
    "hidden_size =  len(embedding_matrix[0]) #25\n",
    "use_dropout=True\n",
    "\n",
    "#model.add(Embedding(vocabulary, hidden_size, input_length=num_steps))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(embedding_matrix), len(embedding_matrix[0]),\n",
    "                    weights=[embedding_matrix], input_length = 150, trainable=False))\n",
    "# model.add(Flatten())\n",
    "\n",
    "#model.add(LSTM(50, return_sequences=True, input_shape=(150, 50)))\n",
    "model.add(LSTM(hidden_size, return_sequences=False))\n",
    "\n",
    "print(model.summary())\n",
    "if use_dropout:\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "#model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# model.add(Dense(8, activation='relu'))\n",
    "# model.add(Dense(1, activation='relu'))\n",
    "# model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='mse', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.loadtxt('embedding_matrix.txt', dtype=float)\n",
    "# hidden_size = 500\n",
    "hidden_size =  len(embedding_matrix[0]) #25\n",
    "use_dropout=True\n",
    "\n",
    "#model.add(Embedding(vocabulary, hidden_size, input_length=num_steps))\n",
    "\n",
    "def part(x):\n",
    "    x0 = x[:3]\n",
    "    x1 = x[3:]\n",
    "    model_2 = Sequential()\n",
    "    model_2.add(Embedding(len(embedding_matrix), len(embedding_matrix[0]),\n",
    "                    weights=[embedding_matrix], input_length = 150, trainable=False))\n",
    "    \n",
    "\n",
    "    #model.add(LSTM(50, return_sequences=True, input_shape=(150, 50)))\n",
    "    model_2.add(LSTM(hidden_size, return_sequences=False))\n",
    "    \n",
    "model = Sequential()\n",
    "model.add(Embedding(len(embedding_matrix), len(embedding_matrix[0]),\n",
    "                    weights=[embedding_matrix], input_length = 150, trainable=False))\n",
    "# model.add(Flatten())\n",
    "\n",
    "#model.add(LSTM(50, return_sequences=True, input_shape=(150, 50)))\n",
    "model.add(LSTM(hidden_size, return_sequences=False))\n",
    "\n",
    "print(model.summary())\n",
    "if use_dropout:\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "#model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# model.add(Dense(8, activation='relu'))\n",
    "# model.add(Dense(1, activation='relu'))\n",
    "# model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='mse', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 116 1119  603 ...    0    0    0]\n",
      " [   1  157 1608 ...    0    0    0]\n",
      " [ 274  185   30 ...    0    0    0]\n",
      " ...\n",
      " [  35 1318  513 ...    0    0    0]\n",
      " [ 124  407   81 ...    0    0    0]\n",
      " [  97   40   41 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "padded_docs = pad_sequences(train_data, maxlen=150, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(np.array(padded_docs), np.array(y_train), epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 53.826800\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(padded_docs, np.array(y_train), verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_word_ids(data, word_to_id):\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = len(words_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(self):\n",
    "    x = np.zeros((self.batch_size, self.num_steps))\n",
    "    y = np.zeros((self.batch_size, self.num_steps, self.vocabulary))\n",
    "    while True:\n",
    "        for i in range(self.batch_size):\n",
    "            if self.current_idx + self.num_steps >= len(self.data):\n",
    "                # reset the index back to the start of the data set\n",
    "                self.current_idx = 0\n",
    "            x[i, :] = self.data[self.current_idx:self.current_idx + self.num_steps]\n",
    "            temp_y = self.data[self.current_idx + 1:self.current_idx + self.num_steps + 1]\n",
    "            # convert all of temp_y into a one hot representation\n",
    "            y[i, :, :] = to_categorical(temp_y, num_classes=self.vocabulary)\n",
    "            self.current_idx += self.skip_step\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 1\n",
    "batch_size = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KerasBatchGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 500\n",
    "num_steps = 50\n",
    "use_dropout=True\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary, hidden_size, input_length=num_steps))\n",
    "model.add(LSTM(hidden_size, return_sequences=True))\n",
    "model.add(LSTM(hidden_size, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(TimeDistributed(Dense(vocabulary)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = Adam()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#output sze 32\n",
    "model.add(Dense(32, activation='relu', input_dim=50))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_vecs_w2v, y_train, epochs=9, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gasia/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  \n",
      "/home/gasia/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(200, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 452820, 50)        3949550   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               200800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 4,150,752\n",
      "Trainable params: 4,150,752\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 50\n",
    "lstm_out = 200\n",
    "batch_size = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_space_df), embed_dim,input_length = len(X_train), dropout = 0.2))\n",
    "model.add(LSTM(lstm_out, dropout_U = 0.2, dropout_W = 0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78991"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_space_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-30db9dd1bddd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # get the data paths\n",
    "    train_path = os.path.join(data_path, \"ptb.train.txt\")\n",
    "    valid_path = os.path.join(data_path, \"ptb.valid.txt\")\n",
    "    test_path = os.path.join(data_path, \"ptb.test.txt\")\n",
    "\n",
    "    # build the complete vocabulary, then convert text data to list of integers\n",
    "    word_to_id = build_vocab(train_path)\n",
    "    train_data = file_to_word_ids(train_path, word_to_id)\n",
    "    valid_data = file_to_word_ids(valid_path, word_to_id)\n",
    "    test_data = file_to_word_ids(test_path, word_to_id)\n",
    "    vocabulary = len(word_to_id)\n",
    "    reversed_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))\n",
    "\n",
    "    print(train_data[:5])\n",
    "    print(word_to_id)\n",
    "    print(vocabulary)\n",
    "    print(\" \".join([reversed_dictionary[x] for x in train_data[:10]]))\n",
    "    return train_data, valid_data, test_data, vocabulary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasBatchGenerator(object):\n",
    "\n",
    "    def __init__(self, data, num_steps, batch_size, vocabulary, skip_step=5):\n",
    "        self.data = data\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary = vocabulary\n",
    "        # this will track the progress of the batches sequentially through the\n",
    "        # data set - once the data reaches the end of the data set it will reset\n",
    "        # back to zero\n",
    "        self.current_idx = 0\n",
    "        # skip_step is the number of words which will be skipped before the next\n",
    "        # batch is skimmed from the data set\n",
    "        self.skip_step = skip_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple arithmetic\n"
     ]
    }
   ],
   "source": [
    "def file_to_word_ids(data, word_to_id):\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]\n",
    "\n",
    "\n",
    "def load_data():\n",
    "\n",
    "    # build the complete vocabulary, then convert text data to list of integers\n",
    "    #word_to_id = build_vocab(train_path)\n",
    "    with open('word_dict.json') as json_data:\n",
    "        word_to_id = json.load(json_data)\n",
    "\n",
    "    #train_data = file_to_word_ids(train_path, word_to_id)\n",
    "    with open('texts_clean.json') as json_data:\n",
    "        all_forum_data = json.load(json_data)\n",
    "    \n",
    "#     all_forum_data =all_forum_data[:20]\n",
    "    ###WARNINGS change to price\n",
    "    random_y = [random.uniform(-1, 1) for i in range(len(all_forum_data))]\n",
    "    price_data = random_y\n",
    "\n",
    "    # divide to train test validation sets (80% 10% 10%)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(all_forum_data, price_data, test_size=0.1, random_state=1)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=1)\n",
    "\n",
    "    train_data = [file_to_word_ids(elem, word_to_id) for elem in X_train]\n",
    "    valid_data = [file_to_word_ids(elem, word_to_id) for elem in X_val  ]\n",
    "    test_data  = [file_to_word_ids(elem, word_to_id) for elem in X_test ]\n",
    "\n",
    "    #valid_data = file_to_word_ids(valid_path, word_to_id)\n",
    "    #test_data = file_to_word_ids(test_path, word_to_id)\n",
    "\n",
    "    vocabulary = len(word_to_id)\n",
    "    reversed_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))\n",
    "\n",
    "    # back to word representation\n",
    "    print(\" \".join([reversed_dictionary[x] for x in train_data[5]]))\n",
    "    return train_data, valid_data, test_data, vocabulary, reversed_dictionary, y_train, y_test, y_val\n",
    "\n",
    "train_data, valid_data, test_data, vocabulary, reversed_dictionary, y_train, y_test, y_val = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
